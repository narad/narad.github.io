<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>

<link href='http://fonts.googleapis.com/css?family=Lato:100,200,300,400,700' rel='stylesheet' type='text/css'>

<title>Jason Naradowsky</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="alternate" type="application/rss+xml" title="RSS 2.0" href="http://www.inference.phy.cam.ac.uk/hmw26/crf/index.rss" />
<script type="text/javascript">
<!--
var hidden = true;

function switchMenu(obj) {
	var el = document.getElementById(obj);
	if ( el.style.display != "none" ) {
		el.style.display = 'none';
	}
	else {
		el.style.display = '';
	}
}

function switchAll(obj, num) {
	if(hidden){
      obj.innerHTML = "hide all abstracts"
	}
	else{
	  obj.innerHTML = "show all abstracts"
	}
    for(var i=1; i<=num; i++){
	  var el = document.getElementById("pub" + i);
	  if ( hidden) {
		el.style.display = '';
	  }
	  else {
		el.style.display = 'none';
	  }
	}
	hidden = !hidden;
}

//-->
</script>

</head>

<body>
<div id="wrap">
<!--
<div class="menu">
Professional | <a href="personal.html"><u>Personal</u></a>
</div>
-->
<div id ="bigtitle">Jason <font color="black">Naradowsky</font></div>
<hr />
<p align="center"><img src="images/colorado3.jpg" width="1000" alt="404" />
<hr />

<div id="wrapLeft">

<h1><a name="about" class="nav-target">about</a></h1>

<p>I'm currently a postdoctoral researcher at the University of Cambridge working with <a href="https://www.cl.cam.ac.uk/~alk23/">Anna Korhonen</a> on the LEXICAL project.&ensp;Previously I was sr. research associate at University College london, working with <a href="http://www.riedelcastro.org/">Sebastian Riedel</a> in the <a href="http://mr.cs.ucl.ac.uk/">Machine Reading lab</a>, and completed my PhD in Computer Science at UMass Amherst with <a href="http://www.ccs.neu.edu/home/dasmith/">David Smith</a> and
<a href="http://web.science.mq.edu.au/~mjohnson/">Mark Johnson</a>.</br>

</br>
My research interests are primarily:
<ul>
<li> Learning latent linguistic structure (morphology, syntax) from unstructured text.</li>
<li> Neural machine translation for low-resource languages. </li>
<li> Building question answering systems that combine dense distributional representations with structured, more symbolic reasoning and priors. </li>
<li> Human-inspired incremental models of NLP via downstream losses (imitation learning, reinforcement learning, etc.). </li>
</ul>

I also try to maintain <a href="http://uclmr.github.io/ai4exams/eqa.html">EQA</a>, a website made for a targetted workshop on exam question answering, but which contains results, publications, and data sets for the recent surge in machine reading QA data sets in an easy-to-digest format.
</p>

Links: &ensp; <a href="cv/narad_cv.pdf">CV</a> &ensp; <a href="http://scholar.google.co.uk/citations?user=w4d5WRcAAAAJ&hl=en&oi=aof">Google Scholar</a> &ensp; <a href="http://umass.academia.edu/JasonNaradowsky">Academia.edu</a> &ensp; <a href="https://github.com/narad">Github</a> </p>

<p>My <a href="http://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93Bacon_number">Erdős–Bacon number</a> is arguably no greater than 8.</p>

<h1><a name="software" class="nav-target">software</a></h1>

<p><a href="http://www.wolfe.ml/">Wolfe</a>:<br />

Wolfe is a probabilistic programming language that enables practitioners to develop machine learning models in a declarative manner.  Wolfe models are written in Scala and compiled by Wolfe into highly-optimized inference and learning routines (using Scala's own abstract syntax trees!), enabling researchers to focus on modelling while Wolfe does the heavy lifting.  It currently features matrix factorization, message passing, and alternating directions dual decomposition, can perform many structured prediction tasks, visualize inference in factor graphs, and more.

<p><a href="http://www.nltk.org/">Natural Language Toolkit (NLTK)</a>:<br />
The Natural Language Toolkit is a collection of open source Python modules that can be used freely for research or pedagogical purposes.  There's also <a href="http://www.amazon.com/Natural-Language-Processing-Python-Steven/dp/0596516495">a book</a> documenting how to use the NTLK which doubles as an introductory computational linguistics coursebook.  For the summer of 2008 I worked on the NLTK while sponsored under the <a href="http://code.google.com/soc/">Google Summer of Code</a> program, during which time I implemented a suite of dependency parsers under the supervision of <a href="http://riedelcastro.github.com/">Sebastian Riedel</a> and <a href="http://www.jasonbaldridge.com/">Jason Baldridge</a>.</p>

</div>

<div id="wrapRight">

</br>
<h1><a name="papers" class="nav-target">papers by year</a><sub style="font-size: 16px;">[<a onclick="switchAll(this, 9);">show all abstracts</a>]</sub></h1>
<hr />
<h2><a name="2016" class="nav-target">2016</a> </h2>

<p><a href="papers/narad_thesis.pdf">Noise reduction and targeted exploration in imitation learning for Abstract Meaning Representation parsing</a><br />
    James Goodman, Andreas Vlachos and Jason Naradowsky<br />
    Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics <br />

    [<a onclick="switchMenu('pub9');">abstract</a>]
    [<a href="http://aclweb.org/anthology/P16-1001">paper</a>]
    [<a href="https://www.dropbox.com/s/c9xtoiuyp80hzc3/goodman2016amr.bib?dl=0">bib</a>]
</p>

<div id="pub9"  style="display:none;">
<blockquote>
	Semantic parsers map natural language
	statements into meaning representations,
	and must abstract over syntactic phenomena,
	resolve anaphora, and identify word
	senses to eliminate ambiguous interpretations.
	Abstract meaning representation
	(AMR) is a recent example of one such
	semantic formalism which, similar to a dependency
	parse, utilizes a graph to represent
	relationships between concepts (Banarescu
	et al., 2013). As with dependency
	parsing, transition-based approaches are a
	common approach to this problem. However,
	when trained in the traditional manner
	these systems are susceptible to the accumulation
	of errors when they find undesirable
	states during greedy decoding.
	Imitation learning algorithms have been
	shown to help these systems recover from
	such errors.
</br>
	To effectively use these methods
	for AMR parsing we find it highly
	beneficial to introduce two novel extensions:
	noise reduction and targeted exploration.
	The former mitigates the noise in
	the feature representation, a result of the
	complexity of the task. The latter targets
	the exploration steps of imitation learning
	towards areas which are likely to provide
	the most information in the context of a
	large action-space. We achieve state-ofthe
	art results, and improve upon standard
	transition-based parsing by 4.7 F1 points
</blockquote>
</div>

<p><a href="papers/narad_thesis.pdf">UCL+Sheffield at SemEval-2016 Task 8: Imitation learning for AMR parsing with an &alpha;-bound</a><br />
    James Goodman, Andreas Vlachos and Jason Naradowsky<br />
    Proceedings of the 10th International Workshop on Semantic Evaluation <br />

    [<a onclick="switchMenu('pub8');">abstract</a>]
    [<a href="https://www.dropbox.com/s/xtval1y2b2w8mi8/naaclhlt2016.pdf?dl=0">paper</a>]
    [<a href="https://www.dropbox.com/s/z4jejb5ka63u9ar/goodman2016emergent.bib?dl=0">bib</a>]
</p>

<div id="pub8"  style="display:none;">
<blockquote>
We  develop  a  novel  transition-based  parsing algorithm for the abstract meaning representation parsing task using exact imitation learning,  in  which  the  parser  learns  a  statisticalmodel  by  imitating  the  actions  of  an  experton the training data.   We then use the imitation learning algorithm DAGGER to improvethe performance, and apply an &alpha;-bound as asimple noise reduction technique. Our performance on the test set was 60% in F-score, andthe performance gains on the development setdue  to  DAGGER was  up  to  1.1  points  of  F-score. The &alpha;-bound improved performance byup to 1.8 points.
</blockquote>
</div>

<hr />
<h2><a name="2014" class="nav-target">2014</a> </h2>

<p><a href="papers/narad_thesis.pdf">Learning with Joint Inference and Latent Linguistic Structure in Graphical Models</a><br />
    Jason Naradowsky<br />
    Doctoral Dissertation, 2014<br />
    Supervisors: <a href="http://www.ccs.neu.edu/home/dasmith/">David Smith</a> and
  <a href="http://web.science.mq.edu.au/~mjohnson/">Mark Johnson</a><br />
    [<a onclick="switchMenu('pub7');">abstract</a>]
    [<a href="papers/narad_thesis.pdf">paper</a>]
    [<a href="bib/narad_thesis.bib">bib</a>]
</p>

<div id="pub7"  style="display:none;">
<blockquote>
    A human listener, charged with the difficult task of mapping language to meaning, must infer a rich hierarchy of linguistic structures, beginning with an utterance and culminating in an understanding of what was spoken. Much in the same manner, developing complete natural language processing systems requires the processing of many different layers of linguistic information in order to solve complex tasks, like answering a query or translating a document.
</br>
</br>
    Historically the community has largely adopted a “divide and conquer” strategy, choosing to split up such complex tasks into smaller fragments which can be tackled independently, with the hope that these smaller contributions will also yield benefits to NLP systems as a whole. These individual components can be laid out in a pipeline and processed in turn, one system’s output becoming input for the next. This approach poses two problems. First, errors propagate, and, much like the childhood game of “telephone”, combining systems in this manner can lead to unintelligible outcomes. Second, each component task requires annotated training data to act as supervision for training the model. These annotations are often expensive and time-consuming to produce, may differ from each other in genre and style, and may not match the intended application.
</br>
</br>
    In this dissertation we pursue novel extensions of joint inference techniques for natural language processing. We present a framework that offers a general method for constructing and performing inference using graphical model formulations of typical NLP problems. Models are composed using weighted Boolean logic constraints, inference is performed using belief propagation. The systems we develop are composed of two parts: one a representation of syntax, the other a desired end task (part-of-speech tagging, semantic role labeling, named entity recognition, or relation extraction). By modeling these problems jointly, both models are trained in a single, integrated process, with uncertainty propagated between them. This mitigates the accumulation of errors typical of pipelined approaches. We further advance previous methods for performing efficient inference on graphical model representations of combinatorial structure, like dependency syntax, extending it to various forms of phrase structure parsing.
</br>
</br>
    Finding appropriate training data is a crucial problem for joint inference models. We observe that in many circumstances, the output of earlier components of the pipeline is often irrelevant – only the end task output is important. Yet we often have strong a priori assumptions regarding what this structure might look like: for phrase structure syntax the model should represent a valid tree, for dependency syntax it should represent a directed graph. We propose a novel marginalization-based training method in which the error signal from end task annotations is used to guide the induction of a constrained latent syntactic representation. This allows training in the absence of syntactic training data, where the latent syntactic structure is instead optimized to best support the end task predictions. We find that across many NLP tasks this training method offers performance comparable to fully supervised training of each individual component, and in some instances im- proves upon it by learning latent structures which are more appropriate for the task.
</blockquote>
</div>

<hr />
<h2><a name="2012" class="nav-target">2012</a> </h2>

<p><a href="papers/relmarg_emnlp2012.pdf">Improving NLP through Marginalization of Hidden Syntactic Structure</a><br />
	Jason Naradowsky,
    <a href="http://riedelcastro.github.com/">Sebastian Riedel</a>, and
    <a href="http://www.ccs.neu.edu/home/dasmith/">David Smith</a>
    <br />
	EMNLP 2012
    <br />
	[<a onclick="switchMenu('pub6');">abstract</a>]
    [<a href="papers/relmarg_emnlp2012.pdf">paper</a>]
    [<a href="bib/narad_emnlp2012.bib">bib</a>]
</p>

<div id="pub6"  style="display:none;">
<blockquote>
	Many NLP tasks make predictions that are inherently coupled to syntactic relations, but for many languages the resources required to provide such syntactic annotations are unavailable.  For others it is unclear exactly how much of the syntactic annotations can be effectively leveraged with current models, and what structures in the syntactic trees are most relevant to the current task.
</br>
</br>
	We propose a novel method which avoids the need for any syntactically annotated data when predicting a related NLP task.  Our method couples latent syntactic representations, constrained to form valid dependency graphs or constituency parses, with the prediction task via specialized factors in a Markov random field.  At both training and test time we marginalize over this hidden structure, learning the optimal latent representations for the problem.  Results show that this approach provides significant gains over a syntactically uninformed baseline, outperforming models that observe syntax on an English relation extraction task, and performing comparably to them in semantic role labeling.
</blockquote>
</div>

<p><a href="papers/narad_coling2012.pdf">Grammarless Parsing for Joint Inference</a><br />
	Jason Naradowsky,
    <a href="http://timvieira.github.com/">Tim Vieira</a>, and
    <a href="http://www.ccs.neu.edu/home/dasmith/">David Smith</a>
    <br />
	COLING 2012
    <br />
	[<a onclick="switchMenu('pub5');">abstract</a>]
    [<a href="papers/narad_coling2012.pdf">paper</a>]
    [<a href="bib/narad_coling2012.bib">bib</a>]
</p>

<div id="pub5"  style="display:none;">
<blockquote>
	Many NLP tasks interact with syntax. The presence of a named entity span, for example, is often a clear indicator of a noun phrase in the parse tree, while a span in the syntax can help indicate the lack of a named entity in the spans that cross it. For these types of problems joint inference offers a better solution than a pipelined approach, and yet large joint models are rarely pursued. In this paper we argue this is due in part to the absence of a general framework for joint inference which can efficiently represent syntactic structure.
</br>
</br>
We propose an alternative and novel method in which constituency parse constraints are imposed on the model via combinatorial factors in a Markov random field, guaranteeing that a variable configuration forms a valid tree. We apply this approach to jointly predicting parse and named entity structure, for which we introduce a zero-order semi-CRF named entity recognizer which also relies on a combinatorial factor. At the junction between these two models, soft constraints coordinate between syntactic constituents and named entity spans, providing an additional layer of flexibility on how these models interact. With this architecture we achieve the best-reported results on both CRF-based parsing and named entity recognition on sections of the OntoNotes corpus, and outperform state-of-the-art parsers on an NP-identification task, while remaining asymptotically faster than traditional grammar-based parsers.
</blockquote>
</div>

<p>Combinatorial Constraints for Constituency Parsing in Graphical Models<br />
	Jason Naradowsky,
    <a href="http://www.ccs.neu.edu/home/dasmith/">David Smith</a>
    <br />
	Technical Report, University of Massachusetts Amherst, 2012.
    <br />
</p>

<hr />
<h2><a name="2011" class="nav-target">2011</a> </h2>

<p><a href="papers/P11-1090.pdf">Unsupervised Bilingual Morpheme Segmentation and Alignment with <br /> Context-rich Hidden Semi-Markov Models</a><br />
	Jason Naradowsky and
    <a href="http://research.microsoft.com/en-us/people/kristout/">Kristina Toutanova</a>
    <br />
	ACL 2011
    <br />
	[<a onclick="switchMenu('pub4');">abstract</a>]
    [<a href="papers/P11-1090.pdf">paper</a>]
    [<a href="slides/morph_align.pdf">slides</a>]
    [<a href="bib/narad_acl11a.bib">bib</a>]
</p>

<!--<br />Slides:[<a href="slides/morph_align.key">keynote</a>][<a href="slides/morph_align.pdf">pdf</a>]</p>
 -->

<div id="pub4"  style="display:none;">
<blockquote>
This paper describes an unsupervised dynamic graphical model for morphological segmentation and bilingual morpheme alignment for statistical machine translation. The model extends Hidden Semi-Markov chain models by using factored output nodes and special structures for its conditional probability distributions. It relies on morpho-syntactic and lexical source-side information (part-of-speech, morphological segmentation) while learning a morpheme segmentation over the target language. Our model outperforms a competitive word alignment system in alignment quality. Used in a monolingual morphological segmentation setting it substantially improves accuracy over previous state-of-the-art models on three Arabic and Hebrew datasets.
</blockquote>
</div>

<p>
	<a href="papers/IR-825.pdf">A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing</a>
	<br />
	<a href="http://www2.ctl.cityu.edu.hk/~jsylee/">John Lee</a>,
    Jason Naradowsky, and
    <a href="http://www.ccs.neu.edu/home/dasmith/">David Smith</a>
    <br />
	ACL 2011
    <br />
	[<a onclick="switchMenu('pub3');">abstract</a>]
    [<a href="papers/IR-825.pdf">paper</a>]
    [<a href="bib/narad_acl11b.bib">bib</a>]
</p>


<div id="pub3"  style="display:none;">
<blockquote>
	Most previous studies of morphological disambiguation and dependency parsing have been pursued independently. Morphological taggers operate on n-grams and do not take into account syntactic relations; parsers use the ``pipeline'' approach, assuming that morphological information has been separately obtained.<br /><br />

	However, in morphologically-rich languages, there is often considerable interaction between morphology and syntax, such that neither can be disambiguated without the other. In this paper, we propose a discriminative model that jointly infers morphological properties and syntactic structures. In evaluations on various highly-inflected languages, this joint model outperforms both a baseline tagger in morphological disambiguation, and a pipeline parser in head selection.
</blockquote>
</div>



<p>
		Feature Induction for Online Constraint-based Phonology Acquisition
	<br />
    Jason Naradowsky,
    <a href="http://people.umass.edu/pater/">Joe Pater</a>, and
    <a href="http://www.ccs.neu.edu/home/dasmith/">David Smith</a>
    <br />
	Synthesis Project, Presented at NECPHON 2011
    <br />
	[<a onclick="switchMenu('pub7');">abstract</a>]
	[<a href="papers/synthesis.pdf">paper</a>]
	[<a href="bib/narad_synthesis.bib">bib</a>]
</p>


<div id="pub7"  style="display:none;">
<blockquote>
Log-linear models provide a convenient method for coupling existing machine learning methods to constraint-based linguistic formalisms like optimality theory and harmonic grammar. While the learning methods themselves have been well studied in this domain, the question of how these constraints originate is often left unanswered. We present a novel, error-driven approach to constraint induction that performs lightweight decisions based on local information. When evaluated on the task of reproducing human gradient phonotactic judgements, a model trained with this procedure can sometimes nearly match the performance of state-of-the-art methods that rely on global information and individual assessment of all possible constraints. We conclude by discussing methods for incorporating context and linguistic bias into the induction scheme to produce more accurate grammars.
</blockquote>
</div>


<hr />
<h2><a name="2010" class="nav-target">2010</a></h2>


<p>Learning Hidden Metrical Structure with a Log-linear Model of Grammar
	<br />
 	Jason Naradowsky,
    <a href="http://people.umass.edu/pater/">Joe Pater</a>,
    <a href="http://www.ccs.neu.edu/home/dasmith/">David Smith</a>, and
    <a href="http://people.umass.edu/rstaubs/">Robert Staubs</a>
    <br />
    Workshop on Computational Modelling of Sound Pattern Acquisition 2010
</p>

<!-- "Learning hidden metrical structure with a log-linear model of grammar." Workshop on Computational Modelling of Sound Pattern Acquisition. Edmonton, Alberta, Canada: University of Alberta. 2010.
 -->

<hr />
<h2><a name="2009" class="nav-target">2009</a></h2>

<p>
	<a href="papers/mimno09polylingual.pdf">Polylingual Topic Models</a>
    <br  />
	<a href="http://www.cs.umass.edu/~mimno/">David Mimno</a>,
    <a href="http://www.cs.umass.edu/~wallach">Hanna Wallach</a>,
    Jason Naradowsky,
    <a href="http://www.ccs.neu.edu/home/dasmith/">David Smith</a> and
	<a href="http://www.cs.umass.edu/~mccallum/">Andrew McCallum</a>
    <br />
    EMNLP 2009
    <br />
    [<a onclick="switchMenu('pub2');">abstract</a>]
    [<a href="papers/mimno09polylingual.pdf">paper</a>]
    [<a href="bib/narad_emnlp09.bib">bib</a>]


<div id="pub2"  style="display:none;">
<blockquote>
Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts. Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages. We introduce a polylingual topic model that discovers topics aligned across multiple languages. We explore the model's characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.
</blockquote>
</div>


<p>
	<a href="papers/ijcai09.pdf">Improving Morphology Induction by Learning Spelling Rules</a>
    <br />
	Jason Naradowsky and
    <a href="http://homepages.inf.ed.ac.uk/sgwater/">Sharon Goldwater</a>
    <br />
    IJCAI 2009
    <br />
    [<a onclick="switchMenu('pub1');">abstract</a>]
    [<a href="papers/ijcai09.pdf">paper</a>]
    [<a href="slides/ijcai09.pdf">slides</a>]
    [<a href="bib/narad_ijcai09.bib">bib</a>]
</p>

<div id="pub1" style="display:none;">
<blockquote>
Unsupervised learning of morphology is an important task for human learners and in natural language processing systems. Previous systems focus on segmenting words into substrings (<em>taking</em> => tak.ing), but sometimes a segmentation-only analysis is insufficient (e.g., <em>taking</em> may be more appropriately analyzed as <em>take.ing</em>, with a spelling rule accounting for the deletion of the stem-final e). In this paper, we develop a Bayesian model for simultaneously inducing both morphology and spelling rules.	We show that the addition of spelling rules improves performance over the baseline morphology-only model.
</blockquote>
</div>

<p>
	<a href="papers/mimno09polylingual.pdf">Polylingual Topic Models</a>
	<br />
	<a href="http://www.cs.umass.edu/~mimno/">David Mimno</a>,
    <a href="http://www.cs.umass.edu/~wallach">Hanna Wallach</a>,
    <a href="http://www.cs.umass.edu/~lmyao/">Limin Yao</a>,
    Jason Naradowsky and
	<a href="http://www.cs.umass.edu/~mccallum/">Andrew McCallum</a>
	<br />
    The Learning Workshop (Snowbird) 2009
    <br />
    </p>

</div>

<div class="footer">
10-10-2016
</div>
</div>
</div>
</body>
</html>
