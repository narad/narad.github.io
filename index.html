<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>

<link href='http://fonts.googleapis.com/css?family=Lato:100,200,300,400,700' rel='stylesheet' type='text/css'>

<title>Jason Naradowsky</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="alternate" type="application/rss+xml" title="RSS 2.0" href="http://www.inference.phy.cam.ac.uk/hmw26/crf/index.rss" />
<script type="text/javascript">
<!--
var hidden = true;

function switchMenu(obj) {
	var el = document.getElementById(obj);
	if ( el.style.display != "none" ) {
		el.style.display = 'none';
	}
	else {
		el.style.display = '';
	}
}

function switchAll(obj, num) {
	if(hidden){
      obj.innerHTML = "hide all abstracts"
	}
	else{
	  obj.innerHTML = "show all abstracts"
	}
    for(var i=1; i<=num; i++){
	  var el = document.getElementById("pub" + i);
	  if ( hidden) {
		el.style.display = '';
	  }
	  else {
		el.style.display = 'none';
	  }
	}
	hidden = !hidden;
}

//-->
</script>

</head>

<body>
<div id="wrap">
<!--
<div class="menu">
Professional | <a href="personal.html"><u>Personal</u></a>
</div>
-->
<div id ="bigtitle">Jason <font color="black">Naradowsky</font></div>
<hr />
<p align="center"><img src="images/colorado3.jpg" width="1000" alt="404" />
<hr />

<div id="wrapLeft">

<h1><a name="about" class="nav-target">about</a></h1>

<p>Hi!  I'm a research scientist at <a href="https://hltcoe.jhu.edu/">Johns Hopkins University</a>, where I specialize in machine learning and natural language processing, and at <a href="https://www.preferred-networks.jp/en/">Preferred Networks</a>, an AI and robotics startup in Tokyo.  Previously I did postdoctoral work at the University of Cambridge (with <a href="https://www.cl.cam.ac.uk/~alk23/">Anna Korhonen</a>) and at University College London (with <a href="http://www.riedelcastro.org/">Sebastian Riedel</a>).  Prior to that I completed my PhD in Computer Science at UMass Amherst with <a href="http://www.ccs.neu.edu/home/dasmith/">David Smith</a> and
<a href="http://web.science.mq.edu.au/~mjohnson/">Mark Johnson</a>.</p>

<p>
I've bounced around a lot, both geographically and topically, and so it is difficult to claim my research has much of a unifying theme, but some overarching trends are:
<ul>
<li> Learning latent linguistic structure (morphology, syntax) from unstructured text.</li>
<li> Neural machine translation for low-resource languages. </li>
<li> Building question answering systems that combine dense distributional representations with structured, more symbolic reasoning and priors. </li>
<li> Human-inspired incremental models of NLP via downstream losses (imitation learning, reinforcement learning, etc.). </li>
</ul>
</p>

<p>Recently I've become increasingly interested in computational creativitiy and the potential for AI in music creation, and perform some consulting work in these areas.  If you're also interested in these topics and would like to pursue a project with me, please feel free to get in touch.</p>

Links: &ensp; <a href="cv/narad_cv.pdf">CV</a> &ensp; <a href="http://scholar.google.co.uk/citations?user=w4d5WRcAAAAJ&hl=en&oi=aof">Google Scholar</a> &ensp; <a href="https://github.com/narad">Github</a> </p>

<p>My <a href="http://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93Bacon_number">Erdős–Bacon number</a> is arguably no greater than 8.</p>

<h1><a name="software" class="nav-target">software</a></h1>

<p><a href="http://www.wolfe.ml/">Wolfe</a>:<br />

Wolfe is a probabilistic programming language that enables practitioners to develop machine learning models in a declarative manner.  Wolfe models are written in Scala and compiled by Wolfe into highly-optimized inference and learning routines (using Scala's own abstract syntax trees!), enabling researchers to focus on modelling while Wolfe does the heavy lifting.  It currently features matrix factorization, message passing, and alternating directions dual decomposition, can perform many structured prediction tasks, visualize inference in factor graphs, and more.

<p><a href="http://www.nltk.org/">Natural Language Toolkit (NLTK)</a>:<br />
The Natural Language Toolkit is a collection of open source Python modules that can be used freely for research or pedagogical purposes.  There's also <a href="http://www.amazon.com/Natural-Language-Processing-Python-Steven/dp/0596516495">a book</a> documenting how to use the NTLK which doubles as an introductory computational linguistics coursebook.  For the summer of 2008 I worked on the NLTK while sponsored under the <a href="http://code.google.com/soc/">Google Summer of Code</a> program, during which time I implemented a suite of dependency parsers under the supervision of <a href="http://riedelcastro.github.com/">Sebastian Riedel</a> and <a href="http://www.jasonbaldridge.com/">Jason Baldridge</a>.</p>

</div>

<div id="wrapRight">

</br>
<h1><a name="papers" class="nav-target">papers by year</a><sub style="font-size: 16px;">[<a onclick="switchAll(this, 16);">show all abstracts</a>]</sub></h1>
<hr />
<h2><a name="2018" class="nav-target">2018</a> </h2>



<p><a href="http://aclweb.org/anthology/Q18-1032">Language Modeling for Morphologically Rich Languages: Character-Aware Modeling for Word-Level Prediction</a><br />
Daniela Gerz, Ivan Vulic´, Edoardo Ponti, Jason Naradowsky, Roi Reichart, and Anna Korhonen1</br>
    TACL 2018<br />

    [<a onclick="switchMenu('pub16');">abstract</a>]
    [<a href="http://aclweb.org/anthology/Q18-1032">paper</a>]
    [<a href="http://aclweb.org/anthology/Q18-1032.bib">bib</a>]
</p>
<div id="pub16" style="display:none;">
<blockquote>
	 Neural architectures are prominent in the construction of language models (LMs). However, word-level prediction is typically agnostic of subword-level information (characters and character sequences) and operates over a closed vocabulary, consisting of a limited word set. Indeed, while subword-aware models boost performance across a variety of NLP tasks, previous work did not evaluate the ability of these models to assist next-word prediction in language modeling tasks. Such subword-level informed models should be particularly effective for morphologically-rich languages (MRLs) that exhibit high type-to-token ratios. In this work, we present a large-scale LM study on 50 typologically diverse languages covering a wide variety of morphological systems, and offer new LM benchmarks to the community, while considering subword-level information. The main technical contribution of our work is a novel method for injecting subword-level information into semantic word vectors, integrated into the neural language modeling training, to facilitate word-level prediction. We conduct experiments in the LM setting where the number of infrequent words is large, and demonstrate strong perplexity gains across our 50 languages, especially for morphologically-rich languages. Our code and data sets are publicly available.
</blockquote>
</div>


<p><a href="https://arxiv.org/abs/1806.03746">A Structured Variational Autoencoder for Morphological Inflection</a><br />
Lawrence Wolf-Sonkin, Jason Naradowsky, Sebastian J. Mielke, Ryan Cotterell </br>
    ACL 2018<br />

    [<a onclick="switchMenu('pub15');">abstract</a>]
    [<a href="papers/structured-variational-autoencoder.pdf">paper</a>]
    [<a href="bib/morph_vae.bib">bib</a>]
</p>
<div id="pub15" style="display:none;">
<blockquote>
	 Statistical morphological inflectors are typically trained on fully
  supervised, type-level data. One remaining open research question is
  the following: How can we effectively exploit raw, token-level data
  to improve their performance? To this end, we introduce a novel
  generative latent-variable  model for the semi-supervised
  learning of inflection generation. To enable posterior inference
  over the latent variables, we derive an efficient variational
  inference procedure based on the wake-sleep algorithm. We experiment
  on 23 languages, using the Universal Dependency corpora in a simulated
  low-resource setting, and find an
  average improvement of 10%.
</blockquote>
</div>

<p><a href="https://arxiv.org/abs/1805.01042">Hypothesis Only Baselines in Natural Language Inference</a><br />
Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme</br>
    *Sem 2018<br />

    [<a onclick="switchMenu('pub14');">abstract</a>]
    [<a href="papers/hypothesis-baselines-natural.pdf">paper</a>]
    [<a href="bib/hypothesis-baselines-natural.bib">bib</a>]
    <br>
    <b>Best Paper Award</b>
</p>
<div id="pub14" style="display:none;">
<blockquote>
We propose a <i>hypothesis only</i> baseline for diagnosing Natural Language Inference (NLI).  Especially when an NLI dataset assumes inference is occurring based purely on the relationship between a context and a hypothesis, it follows that assessing entailment relations while ignoring the provided context is a degenerate solution.  Yet, through experiments on ten distinct NLI datasets, we find that this approach, which we refer to as a hypothesis-only model, is able to significantly outperform a majority-class baseline across a number of NLI datasets.  Our analysis suggests that statistical irregularities may allow a model to perform NLI in some datasets beyond what should be achievable without access to the context.
</blockquote>
</div>

<p><a href="https://arxiv.org/abs/1804.09301">Gender Bias in Coreference Resolution</a><br />
Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme</br>
NAACL 2017<br />

    [<a onclick="switchMenu('pub13');">abstract</a>]
    [<a href="papers/gender-bias-coreference.pdf">paper</a>]
    [<a href="bib/gender-bias-coreference.bib">bib</a>]
</p>
<div id="pub13" style="display:none;">
<blockquote>
We present an empirical study of gender bias in coreference resolution systems. We first introduce a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender.
With these <i>Winogender schemas</i>, we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics.
</blockquote>
</div>




<hr />
<h2><a name="2017" class="nav-target">2017</a> </h2>




<p><a href="https://arxiv.org/pdf/1605.06640.pdf">Programming with a differentiable forth interpreter</a><br />
Matko Bosnjak, Tim Rocktäschel, Jason Naradowsky, and Sebastian Riedel</br>
ICML 2017<br />

    [<a onclick="switchMenu('pub12');">abstract</a>]
    [<a href="https://arxiv.org/pdf/1605.06640.pdf">paper</a>]
    [<a href="bib/bosnjak2016programming.bib">bib</a>]
</p>
<div id="pub12" style="display:none;">
<blockquote>
Given that in practice training data is scarce for all but a
small set of problems, a core question is how to incorporate
prior knowledge into a model. In this paper, we consider
the case of prior <i>procedural</i> knowledge for neural networks,
such as knowing how a program should traverse a sequence,
but not what local actions should be performed at each
step. To this end, we present an end-to-end differentiable
interpreter for the programming language Forth which
enables programmers to write program sketches with slots
that can be filled with behaviour trained from program
input-output data. We can optimise this behaviour directly
through gradient descent techniques on user-specified
objectives, and also integrate the program into any larger
neural computation graph. We show empirically that our
interpreter is able to effectively leverage different levels
of prior program structure and learn complex behaviours
such as sequence sorting and addition. When connected
to outputs of an LSTM and trained jointly, our interpreter
achieves state-of-the-art accuracy for end-to-end reasoning
about quantities expressed in natural language stories.
</blockquote>
</div>

<p><a href="https://deepstruct.github.io/ICML17/1stDeepStructWS_paper_12.pdf">Modeling exclusion with a differentiable factor graph constraint</a><br />
Jason Naradowsky and Sebastian Riedel</br>
ICML 2017, DeepStruct<br />

    [<a onclick="switchMenu('pub11');">abstract</a>]
    [<a href="https://deepstruct.github.io/ICML17/1stDeepStructWS_paper_12.pdf">paper</a>]
    [bib]
</p>
<div id="pub11" style="display:none;">
<blockquote>
With the adoption of general neural network architectures,
many researchers have opted to trade
informative priors for powerful models and big
data. However, for many structured prediction
tasks the complex relationships between variables
in the output space are often difficult to
learn from the available data alone.
<br>
Such relationships often centre around the notion
of exclusion: that predicting one structure
prohibits the prediction of others. In this work
we formulate a differentiable factor graph exclusion
constraint to incorporate this prior belief
into neural end-to-end architectures. We demonstrate
the effectiveness of this method in the context
of extracting event information from clusters
of related news articles, and introduce metainference
learning to determine the ideal number
of inference iterations to simulate.

</blockquote>
</div>

<p><a href="http://aclweb.org/anthology/D17-1220">Break it down for me: A study in automated lyric annotation</a><br />
Lucas Sterckx, Jason Naradowsky, Bill Byrne, Thomas Demeester, and Chris Develder</br>
EMNLP 2017<br />
    [<a onclick="switchMenu('pub10');">abstract</a>]
    [<a href="http://aclweb.org/anthology/D17-1220">paper</a>]
    [<a href="http://aclweb.org/anthology/D17-1220.bib">bib</a>]
</p>
<div id="pub10" style="display:none;">
<blockquote>
Comprehending lyrics, as found in songs and poems, can pose a challenge to
	human and machine readers alike.  This motivates the need for systems that can
	understand the ambiguity and jargon found in such creative texts, and provide
	commentary to aid readers in reaching the correct interpretation.
	We introduce the task of automated lyric annotation (ALA).  Like text
	simplification, a goal of ALA is to rephrase the original text in a more easily
	understandable manner. However, in ALA the system must often include additional
	information to clarify niche terminology and abstract concepts. To stimulate
	research on this task, we release a large collection of crowdsourced
	annotations for song lyrics. We analyze the performance of translation and
	retrieval models on this task, measuring performance with both automated and
	human evaluation. We find that each model captures a unique type of information
	important to the task.
</blockquote>
</div>


<p><a href="papers/rnn_nampi.pdf">A neural forth abstract machine</a><br />
Matko Bosnjak, Tim Rocktäschel, Jason Naradowsky, and Sebastian Riedel</br>
NIPS 2017, NAMPI<br />
    [abstract]
    [<a href="papers/rnn_nampi.pdf">paper</a>]
    [<a href="bib/bosjnak_rnn_nampi.bib">bib</a>]
</p>
<div id="pub14" style="display:none;">
<blockquote>

</blockquote>
</div>


<hr />
<h2><a name="2016" class="nav-target">2016</a> </h2>

<p><a href="http://aclweb.org/anthology/P16-1001">Noise reduction and targeted exploration in imitation learning for Abstract Meaning Representation parsing</a><br />
    James Goodman, Andreas Vlachos and Jason Naradowsky<br />
    Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics <br />

    [<a onclick="switchMenu('pub9');">abstract</a>]
    [<a href="http://aclweb.org/anthology/P16-1001">paper</a>]
    [<a href="https://www.dropbox.com/s/c9xtoiuyp80hzc3/goodman2016amr.bib?dl=0">bib</a>]
</p>

<div id="pub9"  style="display:none;">
<blockquote>
	Semantic parsers map natural language
	statements into meaning representations,
	and must abstract over syntactic phenomena,
	resolve anaphora, and identify word
	senses to eliminate ambiguous interpretations.
	Abstract meaning representation
	(AMR) is a recent example of one such
	semantic formalism which, similar to a dependency
	parse, utilizes a graph to represent
	relationships between concepts (Banarescu
	et al., 2013). As with dependency
	parsing, transition-based approaches are a
	common approach to this problem. However,
	when trained in the traditional manner
	these systems are susceptible to the accumulation
	of errors when they find undesirable
	states during greedy decoding.
	Imitation learning algorithms have been
	shown to help these systems recover from
	such errors.
</br>
	To effectively use these methods
	for AMR parsing we find it highly
	beneficial to introduce two novel extensions:
	noise reduction and targeted exploration.
	The former mitigates the noise in
	the feature representation, a result of the
	complexity of the task. The latter targets
	the exploration steps of imitation learning
	towards areas which are likely to provide
	the most information in the context of a
	large action-space. We achieve state-ofthe
	art results, and improve upon standard
	transition-based parsing by 4.7 F1 points
</blockquote>
</div>

<p><a href="papers/narad_thesis.pdf">UCL+Sheffield at SemEval-2016 Task 8: Imitation learning for AMR parsing with an &alpha;-bound</a><br />
    James Goodman, Andreas Vlachos and Jason Naradowsky<br />
    Proceedings of the 10th International Workshop on Semantic Evaluation <br />

    [<a onclick="switchMenu('pub8');">abstract</a>]
    [<a href="https://www.dropbox.com/s/xtval1y2b2w8mi8/naaclhlt2016.pdf?dl=0">paper</a>]
    [<a href="https://www.dropbox.com/s/z4jejb5ka63u9ar/goodman2016emergent.bib?dl=0">bib</a>]
</p>

<div id="pub8"  style="display:none;">
<blockquote>
We  develop  a  novel  transition-based  parsing algorithm for the abstract meaning representation parsing task using exact imitation learning,  in  which  the  parser  learns  a  statisticalmodel  by  imitating  the  actions  of  an  experton the training data.   We then use the imitation learning algorithm DAGGER to improvethe performance, and apply an &alpha;-bound as asimple noise reduction technique. Our performance on the test set was 60% in F-score, andthe performance gains on the development setdue  to  DAGGER was  up  to  1.1  points  of  F-score. The &alpha;-bound improved performance byup to 1.8 points.
</blockquote>
</div>

<hr />
<h2><a name="2014" class="nav-target">2014</a> </h2>

<p><a href="papers/narad_thesis.pdf">Learning with Joint Inference and Latent Linguistic Structure in Graphical Models</a><br />
    Jason Naradowsky<br />
    Doctoral Dissertation, 2014<br />
    Supervisors: <a href="http://www.ccs.neu.edu/home/dasmith/">David Smith</a> and
  <a href="http://web.science.mq.edu.au/~mjohnson/">Mark Johnson</a><br />
    [<a onclick="switchMenu('pub7');">abstract</a>]
    [<a href="papers/narad_thesis.pdf">paper</a>]
    [<a href="bib/narad_thesis.bib">bib</a>]
</p>

<div id="pub7"  style="display:none;">
<blockquote>
    A human listener, charged with the difficult task of mapping language to meaning, must infer a rich hierarchy of linguistic structures, beginning with an utterance and culminating in an understanding of what was spoken. Much in the same manner, developing complete natural language processing systems requires the processing of many different layers of linguistic information in order to solve complex tasks, like answering a query or translating a document.
</br>
</br>
    Historically the community has largely adopted a “divide and conquer” strategy, choosing to split up such complex tasks into smaller fragments which can be tackled independently, with the hope that these smaller contributions will also yield benefits to NLP systems as a whole. These individual components can be laid out in a pipeline and processed in turn, one system’s output becoming input for the next. This approach poses two problems. First, errors propagate, and, much like the childhood game of “telephone”, combining systems in this manner can lead to unintelligible outcomes. Second, each component task requires annotated training data to act as supervision for training the model. These annotations are often expensive and time-consuming to produce, may differ from each other in genre and style, and may not match the intended application.
</br>
</br>
    In this dissertation we pursue novel extensions of joint inference techniques for natural language processing. We present a framework that offers a general method for constructing and performing inference using graphical model formulations of typical NLP problems. Models are composed using weighted Boolean logic constraints, inference is performed using belief propagation. The systems we develop are composed of two parts: one a representation of syntax, the other a desired end task (part-of-speech tagging, semantic role labeling, named entity recognition, or relation extraction). By modeling these problems jointly, both models are trained in a single, integrated process, with uncertainty propagated between them. This mitigates the accumulation of errors typical of pipelined approaches. We further advance previous methods for performing efficient inference on graphical model representations of combinatorial structure, like dependency syntax, extending it to various forms of phrase structure parsing.
</br>
</br>
    Finding appropriate training data is a crucial problem for joint inference models. We observe that in many circumstances, the output of earlier components of the pipeline is often irrelevant – only the end task output is important. Yet we often have strong a priori assumptions regarding what this structure might look like: for phrase structure syntax the model should represent a valid tree, for dependency syntax it should represent a directed graph. We propose a novel marginalization-based training method in which the error signal from end task annotations is used to guide the induction of a constrained latent syntactic representation. This allows training in the absence of syntactic training data, where the latent syntactic structure is instead optimized to best support the end task predictions. We find that across many NLP tasks this training method offers performance comparable to fully supervised training of each individual component, and in some instances im- proves upon it by learning latent structures which are more appropriate for the task.
</blockquote>
</div>

<hr />
<h2><a name="2012" class="nav-target">2012</a> </h2>

<p><a href="papers/relmarg_emnlp2012.pdf">Improving NLP through Marginalization of Hidden Syntactic Structure</a><br />
	Jason Naradowsky,
    <a href="http://riedelcastro.github.com/">Sebastian Riedel</a>, and
    <a href="http://www.ccs.neu.edu/home/dasmith/">David Smith</a>
    <br />
	EMNLP 2012
    <br />
	[<a onclick="switchMenu('pub6');">abstract</a>]
    [<a href="papers/relmarg_emnlp2012.pdf">paper</a>]
    [<a href="bib/narad_emnlp2012.bib">bib</a>]
</p>

<div id="pub6"  style="display:none;">
<blockquote>
	Many NLP tasks make predictions that are inherently coupled to syntactic relations, but for many languages the resources required to provide such syntactic annotations are unavailable.  For others it is unclear exactly how much of the syntactic annotations can be effectively leveraged with current models, and what structures in the syntactic trees are most relevant to the current task.
</br>
</br>
	We propose a novel method which avoids the need for any syntactically annotated data when predicting a related NLP task.  Our method couples latent syntactic representations, constrained to form valid dependency graphs or constituency parses, with the prediction task via specialized factors in a Markov random field.  At both training and test time we marginalize over this hidden structure, learning the optimal latent representations for the problem.  Results show that this approach provides significant gains over a syntactically uninformed baseline, outperforming models that observe syntax on an English relation extraction task, and performing comparably to them in semantic role labeling.
</blockquote>
</div>

<p><a href="papers/narad_coling2012.pdf">Grammarless Parsing for Joint Inference</a><br />
	Jason Naradowsky,
    <a href="http://timvieira.github.com/">Tim Vieira</a>, and
    <a href="http://www.ccs.neu.edu/home/dasmith/">David Smith</a>
    <br />
	COLING 2012
    <br />
	[<a onclick="switchMenu('pub5');">abstract</a>]
    [<a href="papers/narad_coling2012.pdf">paper</a>]
    [<a href="bib/narad_coling2012.bib">bib</a>]
</p>

<div id="pub5"  style="display:none;">
<blockquote>
	Many NLP tasks interact with syntax. The presence of a named entity span, for example, is often a clear indicator of a noun phrase in the parse tree, while a span in the syntax can help indicate the lack of a named entity in the spans that cross it. For these types of problems joint inference offers a better solution than a pipelined approach, and yet large joint models are rarely pursued. In this paper we argue this is due in part to the absence of a general framework for joint inference which can efficiently represent syntactic structure.
</br>
</br>
We propose an alternative and novel method in which constituency parse constraints are imposed on the model via combinatorial factors in a Markov random field, guaranteeing that a variable configuration forms a valid tree. We apply this approach to jointly predicting parse and named entity structure, for which we introduce a zero-order semi-CRF named entity recognizer which also relies on a combinatorial factor. At the junction between these two models, soft constraints coordinate between syntactic constituents and named entity spans, providing an additional layer of flexibility on how these models interact. With this architecture we achieve the best-reported results on both CRF-based parsing and named entity recognition on sections of the OntoNotes corpus, and outperform state-of-the-art parsers on an NP-identification task, while remaining asymptotically faster than traditional grammar-based parsers.
</blockquote>
</div>

<p>Combinatorial Constraints for Constituency Parsing in Graphical Models<br />
	Jason Naradowsky,
    <a href="http://www.ccs.neu.edu/home/dasmith/">David Smith</a>
    <br />
	Technical Report, University of Massachusetts Amherst, 2012.
    <br />
</p>

<hr />
<h2><a name="2011" class="nav-target">2011</a> </h2>

<p><a href="papers/P11-1090.pdf">Unsupervised Bilingual Morpheme Segmentation and Alignment with <br /> Context-rich Hidden Semi-Markov Models</a><br />
	Jason Naradowsky and
    <a href="http://research.microsoft.com/en-us/people/kristout/">Kristina Toutanova</a>
    <br />
	ACL 2011
    <br />
	[<a onclick="switchMenu('pub4');">abstract</a>]
    [<a href="papers/P11-1090.pdf">paper</a>]
    [<a href="slides/morph_align.pdf">slides</a>]
    [<a href="bib/narad_acl11a.bib">bib</a>]
</p>

<!--<br />Slides:[<a href="slides/morph_align.key">keynote</a>][<a href="slides/morph_align.pdf">pdf</a>]</p>
 -->

<div id="pub4"  style="display:none;">
<blockquote>
This paper describes an unsupervised dynamic graphical model for morphological segmentation and bilingual morpheme alignment for statistical machine translation. The model extends Hidden Semi-Markov chain models by using factored output nodes and special structures for its conditional probability distributions. It relies on morpho-syntactic and lexical source-side information (part-of-speech, morphological segmentation) while learning a morpheme segmentation over the target language. Our model outperforms a competitive word alignment system in alignment quality. Used in a monolingual morphological segmentation setting it substantially improves accuracy over previous state-of-the-art models on three Arabic and Hebrew datasets.
</blockquote>
</div>

<p>
	<a href="papers/IR-825.pdf">A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing</a>
	<br />
	<a href="http://www2.ctl.cityu.edu.hk/~jsylee/">John Lee</a>,
    Jason Naradowsky, and
    <a href="http://www.ccs.neu.edu/home/dasmith/">David Smith</a>
    <br />
	ACL 2011
    <br />
	[<a onclick="switchMenu('pub3');">abstract</a>]
    [<a href="papers/IR-825.pdf">paper</a>]
    [<a href="bib/narad_acl11b.bib">bib</a>]
</p>


<div id="pub3"  style="display:none;">
<blockquote>
	Most previous studies of morphological disambiguation and dependency parsing have been pursued independently. Morphological taggers operate on n-grams and do not take into account syntactic relations; parsers use the ``pipeline'' approach, assuming that morphological information has been separately obtained.<br /><br />

	However, in morphologically-rich languages, there is often considerable interaction between morphology and syntax, such that neither can be disambiguated without the other. In this paper, we propose a discriminative model that jointly infers morphological properties and syntactic structures. In evaluations on various highly-inflected languages, this joint model outperforms both a baseline tagger in morphological disambiguation, and a pipeline parser in head selection.
</blockquote>
</div>



<p>
		Feature Induction for Online Constraint-based Phonology Acquisition
	<br />
    Jason Naradowsky,
    <a href="http://people.umass.edu/pater/">Joe Pater</a>, and
    <a href="http://www.ccs.neu.edu/home/dasmith/">David Smith</a>
    <br />
	Synthesis Project, Presented at NECPHON 2011
    <br />
	[<a onclick="switchMenu('pub7');">abstract</a>]
	[<a href="papers/synthesis.pdf">paper</a>]
	[<a href="bib/narad_synthesis.bib">bib</a>]
</p>


<div id="pub7"  style="display:none;">
<blockquote>
Log-linear models provide a convenient method for coupling existing machine learning methods to constraint-based linguistic formalisms like optimality theory and harmonic grammar. While the learning methods themselves have been well studied in this domain, the question of how these constraints originate is often left unanswered. We present a novel, error-driven approach to constraint induction that performs lightweight decisions based on local information. When evaluated on the task of reproducing human gradient phonotactic judgements, a model trained with this procedure can sometimes nearly match the performance of state-of-the-art methods that rely on global information and individual assessment of all possible constraints. We conclude by discussing methods for incorporating context and linguistic bias into the induction scheme to produce more accurate grammars.
</blockquote>
</div>


<hr />
<h2><a name="2010" class="nav-target">2010</a></h2>


<p>Learning Hidden Metrical Structure with a Log-linear Model of Grammar
	<br />
 	Jason Naradowsky,
    <a href="http://people.umass.edu/pater/">Joe Pater</a>,
    <a href="http://www.ccs.neu.edu/home/dasmith/">David Smith</a>, and
    <a href="http://people.umass.edu/rstaubs/">Robert Staubs</a>
    <br />
    Workshop on Computational Modelling of Sound Pattern Acquisition 2010
</p>

<!-- "Learning hidden metrical structure with a log-linear model of grammar." Workshop on Computational Modelling of Sound Pattern Acquisition. Edmonton, Alberta, Canada: University of Alberta. 2010.
 -->

<hr />
<h2><a name="2009" class="nav-target">2009</a></h2>

<p>
	<a href="papers/mimno09polylingual.pdf">Polylingual Topic Models</a>
    <br  />
	<a href="http://www.cs.umass.edu/~mimno/">David Mimno</a>,
    <a href="http://www.cs.umass.edu/~wallach">Hanna Wallach</a>,
    Jason Naradowsky,
    <a href="http://www.ccs.neu.edu/home/dasmith/">David Smith</a> and
	<a href="http://www.cs.umass.edu/~mccallum/">Andrew McCallum</a>
    <br />
    EMNLP 2009
    <br />
    [<a onclick="switchMenu('pub2');">abstract</a>]
    [<a href="papers/mimno09polylingual.pdf">paper</a>]
    [<a href="bib/narad_emnlp09.bib">bib</a>]


<div id="pub2"  style="display:none;">
<blockquote>
Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts. Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages. We introduce a polylingual topic model that discovers topics aligned across multiple languages. We explore the model's characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.
</blockquote>
</div>


<p>
	<a href="papers/ijcai09.pdf">Improving Morphology Induction by Learning Spelling Rules</a>
    <br />
	Jason Naradowsky and
    <a href="http://homepages.inf.ed.ac.uk/sgwater/">Sharon Goldwater</a>
    <br />
    IJCAI 2009
    <br />
    [<a onclick="switchMenu('pub1');">abstract</a>]
    [<a href="papers/ijcai09.pdf">paper</a>]
    [<a href="slides/ijcai09.pdf">slides</a>]
    [<a href="bib/narad_ijcai09.bib">bib</a>]
</p>

<div id="pub1" style="display:none;">
<blockquote>
Unsupervised learning of morphology is an important task for human learners and in natural language processing systems. Previous systems focus on segmenting words into substrings (<em>taking</em> => tak.ing), but sometimes a segmentation-only analysis is insufficient (e.g., <em>taking</em> may be more appropriately analyzed as <em>take.ing</em>, with a spelling rule accounting for the deletion of the stem-final e). In this paper, we develop a Bayesian model for simultaneously inducing both morphology and spelling rules.	We show that the addition of spelling rules improves performance over the baseline morphology-only model.
</blockquote>
</div>

<p>
	<a href="papers/mimno09polylingual.pdf">Polylingual Topic Models</a>
	<br />
	<a href="http://www.cs.umass.edu/~mimno/">David Mimno</a>,
    <a href="http://www.cs.umass.edu/~wallach">Hanna Wallach</a>,
    <a href="http://www.cs.umass.edu/~lmyao/">Limin Yao</a>,
    Jason Naradowsky and
	<a href="http://www.cs.umass.edu/~mccallum/">Andrew McCallum</a>
	<br />
    The Learning Workshop (Snowbird) 2009
    <br />
    </p>

</div>

<div class="footer">
10-10-2016
</div>
</div>
</div>
</body>
</html>
